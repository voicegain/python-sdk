# coding: utf-8

# flake8: noqa

"""
    Voicegain API v1

    # New  [Telephony Bot API](#tag/aivr-callback) for building interactive speech-enabled phone applications  (IVR, Voicebots, etc.).    # Intro to Voicegain APIs The APIs are provided by [Voicegain](https://www.voicegain.ai) to its registered customers. </br> The core APIs are for Speech-to-Text (STT), either transcription or recognition (further described in next Sections).</br> Other available APIs include: + Telephony Bot APIs which in addition to speech-to-text allow for control of real-time communications (RTC) session (e.g., a telephone call). + Websocket APIs for managing broadcast websockets used in real-time transcription. + Language Model creation and manipulation APIs. + Data upload APIs that help in certain STT use scenarios. + Speech Analytics APIs (currently in **beta**) + Training Set APIs - for use in preparing data for acoustic model training. + GREG APIs - for working with ASR and Grammar tuning tool - GREG. + Security APIs.   Python SDK for this API is available at [PyPI Repository](https://pypi.org/project/voicegain-speech/)  In addition to this API Spec document please also consult our Knowledge Base Articles: * [Web API Section](https://support.voicegain.ai/hc/en-us/categories/360001288691-Web-API) of our Knowledge Base   * [Authentication for Web API](https://support.voicegain.ai/hc/en-us/sections/360004485831-Authentication-for-Web-API) - how to generate and use JWT   * [Basic Web API Use Cases](https://support.voicegain.ai/hc/en-us/sections/360004660632-Basic-Web-API-Use-Cases)   * [Example applications using Voicegain API](https://support.voicegain.ai/hc/en-us/sections/360009682932-Example-applications-using-Voicegain-API)  **NOTE:** Most of the request and response examples in this API document are generated from schema example annotation. This may result in the response example not matching the request data example.</br> We will be adding specific examples to certain API methods if we notice that this is needed to clarify the usage.  # JWT Authentication Almost all methods from this API require authentication by means of a JWT Token. A valid token can be obtained from the [Voicegain Web Console](https://console.voicegain.ai).   Each Context within the Account has its own JWT token. The accountId and contextId are encoded inside the token,  that is why API method requests do not require these in their request parameters.  When making Web APi request the JWT has to be included in the \"Authorization: Bearer\" header. For example, when using curl to make a request:  <pre>   curl -i -X POST \\   -H \"Content-Type: application/json\" \\   -H \"Accept: application/json\" \\   -H \"Authorization: Bearer eyJh......BOGCO70w\" \\   -d @data1.json \\   https://api.voicegain.ai/v1/asr/transcribe/async </pre>  More information about generating and using JWT with Voicegain API can be found in our  [Support Pages](https://support.voicegain.ai/hc/en-us/articles/360028023691-JWT-Authentication).  # Edge Deployment API URLs  When you are using Voicegain plaform deployed on Edge, the Web API urls will be different from those that are used in the Cloud (and given in the examples).  For example: * a Web API URL in the Cloud may be: https://api.voicegain.ai/v1/asr/transcribe/async  * but when deployed on Edge which e.g. has this IP:port 10.137.16.7:31680 and does not have SSL configured   * the URL for the same API will be http://10.137.16.7:31680/ascalon-web-api/asr/transcribe/async  * if deployed on Edge with SSL cert and IP:port 10.137.16.7:31443   * the URL for the same API will be https://10.137.16.7:31443/ascalon-web-api/asr/transcribe/async  The reason for this is that in the Cloud, the Web API service is on its own hostname, but on the Edge it has to share the hostname/IP with the Web Console  (which would e.g. have this URL: https://10.137.16.7:31443/customer-portal/)  # Context Defaults  Most of the API requests are made within a specific Context identified by the JWT being used. Each Context has some API (mainly ASR API) related settings which can be set from the Web Console, see image below: ![Context Settings](https://github.com/voicegain/platform/raw/master/images/Context-Speech-Recognition-Settings.PNG)  These settings override the corresponding API default values.  For example, if `noInputTimeout` default is 15000, but the Context 'No Input Timeout' setting is 30000,  and no value is provided in the API request for `noInputTimeout` field, then the API request will run with `noInputTimeout` of 30000.    # Transcribe API  **/asr/transcribe**</br> The Transcribe API allows you to submit audio and receive the transcribed text word-for-word from the STT engine.  This API uses our Large Vocabulary language model and supports long form audio in async mode. </br> The API can, e.g., be used to transcribe audio data - whether it is podcasts, voicemails, call recordings, etc.  In real-time streaming mode it can, e.g., be used for building voice-bots (your the application will have to provide NLU capabilities to determine intent from the transcribed text).    The result of transcription can be returned in four formats. These are requested inside session[].content when making initial transcribe request:  + **Transcript** - Contains the complete text of transcription + **Words** - Intermediate results will contain new words, with timing and confidences, since the previous intermediate result. The final result will contain complete transcription. + **Word-Tree** - Contains a tree of all feasible alternatives. Use this when integrating with NL postprocessing to determine the final utterance and its meaning. + **Captions** - Intermediate results will be suitable to use as captions (this feature is in beta).  # Recognize API  **/asr/recognize**</br> This API should be used if you want to constrain STT recognition results to the speech-grammar that is submitted along with the audio  (grammars are used in place of large vocabulary language model).</br> While building grammars can be time-consuming step, they can simplify the development of applications since the semantic  meaning can be extracted along with the text. </br> Voicegain supports grammars in the JSGF and GRXML formats â€“ both grammar standards used by enterprises in IVRs since early 2000s.</br> The recognize API only supports short form audio - no more than 30 seconds.   # Sync/Async Mode  Speech-to-Text APIs can be accessed in two modes:  + **Sync mode:**  This is the default mode that is invoked when a client makes a request for the Transcribe (/asr/transcribe) and Recognize (/asr/recognize) urls.</br> A Speech-to-Text API synchronous request is the simplest method for performing processing on speech audio data.  Speech-to-Text can process up to 1 minute of speech audio data sent in a synchronous request.  After Speech-to-Text processes all of the audio, it returns a response.</br> A synchronous request is blocking, meaning that Speech-to-Text must return a response before processing the next request.  Speech-to-Text typically processes audio faster than realtime.</br> For longer audio please use Async mode.    + **Async Mode:**  This is invoked by adding the /async to the Transcribe and Recognize url (so /asr/transcribe/async and /asr/recognize/async respectively). </br> In this mode the initial HTTP request request will return as soon as the STT session is established.  The response will contain a session id which can be used to obtain either incremental or full result of speech-to-text processing.  In this mode, the Voicegain platform can provide multiple intermediate recognition/transcription responses to the client as they become available before sending a final response.  ## Async Sessions: Real-Time, Semi Real-Time, and Off-Line  There are 3 types of Async ASR session that can be started:  + **REAL-TIME** - Real-time processing of streaming audio. For the recognition API, results are available within less than one second after end of utterance.  For the transcription API, real-time incremental results will be sent back with under 1 seconds delay.  + **OFF-LINE** - offline transcription or recognition. Has higher accuracy than REAL-TIME. Results are delivered once the complete audio has been processed.  Currently, 1 hour long audio is processed in about 10 minutes. + **SEMI-REAL-TIME** - Similar in use to REAL-TIME, but the results are available with a delay of about 30-45 seconds (or earlier for shorter audio).  Same accuracy as OFF-LINE.  It is possible to start up to 2 simultaneous sessions attached to the same audio.   The allowed combinations of the types of two sessions are:  + REAL-TIME + SEMI-REAL-TIME - one possible use case is a combination of live transcription with transcription for online streaming (which may be delayed w.r.t of real time). The benefit of using separate SEMI-REAL-TIME session is that it has higher accuracy. + REAL-TIME + OFF-LINE - one possible use case is combination of live transcription with higher quality off-line transcription for archival purposes. + 2x REAL-TIME - for example for separately transcribing left and right channels of stereo audio  Other combinations of session types, including more than 2 sessions, are currently not supported.  Specifically, 2x OFF-LINE use case is not supported because of how the task queue processor is implemented. To transcribe 2-channels separately in OFF-LINE mode you will need to make 2 separate OFF-LINE transcription requests. Please, let us know if you think you have a valid use case for other combinations.  # Telephony Bot API  (previously called RTC Callback API, where RTC stands for Real Time Communications)   Voicegain Telephony Bot APIs allows you to build conversational voice-enabled applications (e.g. IVRs, Voicebots) over an RTC session (a telephone call for example).  See this blog post for an overview of how this API works: [Voicegain releases Telephony Bot APIs for telephony IVRs and bots](https://www.voicegain.ai/post/rtc-callback-api-released)  Telephony Bot API is a callback API - Voicegain platform makes HTTP request to your app with information about the result of e.g. latest recognition and in response you provide instruction for the next step of the conversation. See the spec of these requests [here](#tag/aivr-callback).  # Speech Analytics API  Voicegain Speech Analytics analyzes both the transcript and the audio (typically of a telephone call).  The results are returned per channel (real or diarized) except where the recognized entities span more than one channel. For entities where it is applicable we return the location in the audio (start and end time) and the transcript (index of the words).  ## Capabilities of Speech Analytics  Voicegain Speech Analytics can identify/compute the following: + **named entities** - (NER i.e. named entity recognition) - the following entities are recognized:   + ADDRESS - Postal address.   + CARDINAL - Numerals that do not fall under another type.   + CC - Credit Card   + DATE - Absolute or relative dates or periods.   + DMY - Full date including all of day, month and year.         + EMAIL - Email address   + EVENT - Named hurricanes, battles, wars, sports events, etc.   + FAC - Buildings, airports, highways, bridges, etc.   + GPE - Countries, cities, states.   + LANGUAGE - Any named language.   + LAW - Named documents made into laws.   + NORP - Nationalities or religious or political groups.   + MONEY - Monetary values, including unit.   + ORDINAL - \"first\", \"second\", etc.   + ORG - Companies, agencies, institutions, etc.   + PERCENT - Percentage, including \"%\".   + PERSON - People, including fictional.   + PHONE - Phone number.   + PRODUCT - Objects, vehicles, foods, etc. (Not services.)   + QUANTITY - Measurements, as of weight or distance.   + SSN - Social Security number   + TIME - Times smaller than a day.   + WORK_OF_ART - Titles of books, songs, etc.   + ZIP - Zip Code (if not part of an Address)    In addition to returning the named entity itself, we return the sub-concepts within entity, e.g. for ADDRESs we will return state (e.g. TX) and zip code if found.  + **keywords** - these are single words or short phrases e.g. company or product names.    Currently, keywords are detected using simple matching using stemming - so e.g. a keyword \"cancel\" will match \"cancellation\".    In near future we will support \"smart expansion\" which will also match synonyms while paying attention to the correct meaning of the word.     In addition to keywords we return keyword groups, e.g. several company name keywords can be combined into a `Competition` keyword group.  + **phrases (intent)** - allows for detection of phrases/intents that match the meaning of the phrases specified in the example training Sections).</br>   For each detected phrase/intent the system will also return entities and keywords contained in the phrase, if configured to do so.   For example, transcript \"Hello, my name is Lucy\" may match phrase/intent \"INTRODUCTION\" with the NER of PERSON and value \"Lucy\".       The configuration for phrase/intent detection takes the following parameters:   + _list_ of example phrases - each phrase has a sensitivity value which determines how close it has to match (sensitivity of 1.0 requires the closest match, sensitivity of 0.0 allows for vague matches).   + _regex_ - optional regex phrases to augment the examples - these require exact match   + _slots_ - types on named entities and keywords to be recognized within the phrase/intent</br>     Note: support for slots of same type but different meaning will be added in the future.     Currently it is possible e.g. to recognize places (GPE) but not possible to distinguish e.g. between types of them, like departure or destination place.   + _location_ - this narrows down where the phrase/match must occur - the options are:     + channel - agent or caller      + time in the call - from the start or from the end     + dialogue act - require the phrase to be part of a specified dialogue act, see https://web.stanford.edu/~jurafsky/ws97/manual.august1.html, first table, column SWBD    + **phrase groups** - computed across all channels - this is more powerful than keyword groups as it can be configured to require all phrases/intents in the groups to be present in any or fixed order.   One use case would be to detect a pair of a question and a confirming answer - for example to determine call resolution: \"Have I answered all your question?\", \"Yes\". + **criteria** - computed by rules/conditions looking at the following parameters:   + _call metrics_   + _regex_ - match of the text of the transcript   + _keywords_ - any keywords or keyword groups   + _NER_ - any named entities   + _phrases_ - any phrases/intents or phrase groups   + _dialogElements_ - selection of custom hardcoded rules that may accomplish tasks not possible with other conditions    The individual rules/conditions can be further narrowed down using filters like:   + _channel_ - agent or caller    + _time in the call_ - from the start or from the end    Multiple rules can be combined to form a logical AND expression.   Finally, the individual rules can be negated so that the absence of certain events is considered as a positive match.    When Criteria are satisfied then the system provides a detailed justification information. + **topics** - computed from text across all channels - assigns to the call a set of likely topics with their scores.    A topic classifier is built in a separate step using a corpus. The build process requires manual labeling of the topics.    For each call, the entire transcript is fed to the topic classifier and we get back the set of detected topics and their scores (in the 0..1 range).   It is useful e.g. for separating Billing calls from Troubleshooting calls from Account Change calls, etc.  + **summary** - computed from text across all channels - provides a summary of the call in a form of a set of sentences.   These may either be key sentences directly pulled from the transcript, or sentences generated by summarizing entire call or sections of the call.  + **sentiment** - computed from text - standard call sentiment as used in Call Center Speech Analytics.   Returns sentiment values from -1.0 (negative/mad/angry) to +1.0 (positive/happy/satisfied) + **mood** - computed from text - can distinguish 6 moods:   + neutral    + anger    + disgust    + fear    + happiness   + sadness   + surprise     Values are returned as a map from mood enum values to a number in (0.0, 1.0) range - multiple moods can be detected in the same place in the transcript in varying degrees. + **gender** - computed to audio - Estimates the gender of the speaker as far as it is possible to do it from the voice alone. + **word cloud** - returns word cloud data (map from words/phrases to frequencies) - the algorithm uses: stop word removal, stemming, frequent phrase detection. + **call metrics** - these are simple metrics computed from the audio and the transcript    + _silence_ - amount of silence in the call   + _talk_ - talk streaks for each of the channels   + _overtalk_ - amount of time when call participants talk over ove another   + _energy_ - the volume of the call and the variation   + _pitch_ - the pitch (frequency of the voice) and the variation  Voicegain allows for configuring Speech Analytics processing by preparing a Speech Analytics Configuration which is basically a selection of the capabilities mentioned above plus configuration of variable elements like keywords, phrases, etc.  </br> You can configure Speech Analytics using **[/sa/config API](#operation/saConfigPost)**   Once the configuration is complete you can launch speech transcription and analytics session using the **[/sa API](#operation/saPost)**   ## Offline vs Real-Time Speech Analytics  Speech audio can be transcribed and then analyzed in one of two modes: + **OFF-LINE** - use the `/sa/offline/` API for this.    Audio will be queued for transcription, then transcribed, and both the audio and transcript will pass through various speech analytics algorithms according to the specified configuration.   The results of transcription and speech analytics can be retrieved using the [GET **/sa/offline/{sid}/data** API](#tag/sa-offline/operation/saOfflineGetData)   + **REAL-TIME** - use the `/sa` API for this.    Audio will immediately be submitted to real-time transcription and the stream of transcribed words will be fed to real-time speech analytics.    The results of transcription and speech analytics will be returned over websocket as soon as they are available. </br>   The format of the returned messages is defined [here](#operation/saWebsocketPayload).    Note that not all speech analytics features are available in real-time. Features missing in real-time are: criteria, topics, summary, gender, word cloud, and call metrics.</br>   The results will also be available afterwards using the [GET **/sa/{sid}/data** API](#operation/saDataGet)  ## Agent Review Form  Data computed by Speech Analytics can be used to automatically fill/answer questions of the Call/Agent Review Form.   The automatic answers can be obtained based on previously defined Criteria (see above).  When Criteria are satisfied then the system provides a detailed justification information so it is easily possible to verify that the automated answer on a Review Form was correct.  ## PII Redaction  Being able to recognize occurrence of certain elements in the transcript allows us to remove them from both the text and the audio - this is called PII Redaction where PII stands for Personally Identifiable Information.  Currently, PII Redaction is limited to named entities (NER).  User can select any NER type detected by [Speech Analytics](#section/Speech-Analytics-API/Capabilities-of-Speech-Analytics) to be replaced by a specified placeholder in the text and by silence in the audio.  If your Enterprise account with Voicegain is setup with PCI-DSS compliance option, then PII Redaction of credit card numbers is enabled by default and cannot be disabled.    # Audio Input  The speech audio can be submitted in variety of ways:  + **Inline** - Short audio data can be encoded inside a request as a base64 string. + **Retrieved from URL** - Audio can be retrieved from a provided URL. The URL can also point to a live stream. + **Streamed via RTP** - Recommended only for Edge use cases (not for Cloud). + **Streamed via proprietary UDP protocol** - We provide a Java utility to do this. The utility can stream directly from an audio device, or from a file. + **Streamed via Websocket** - Can be used, e.g., to do microphone capture directly from the web browser. + **From Object Store** - Currently it works only with files uploaded to Voicegain object store, but will be expanded to support other Object Stores.  # Rate Limiting  Access to Voicegain resources is controlled using the following limit settings on the account.  Newly created accounts get the limit values listed below.  If you need higher limits please contact us at support@voicegain.ai  The limits apply to the use of the Voicegain Platform in the Cloud.  On the Edge, the limits will be determined by the type of license you will purchase.  ## Types of Rate Limits  | Limit | default value | description | |---|---|---| | apiRequestLimitPerMinute | 75 | Basic rate limit with a fixed window of 1 minute applying to all API requests. Requests to /data API will be counted at 10x other requests. | | apiRequestLimitPerHour | 2000 | Basic rate limit with a fixed window of 1 hour applying to all API requests. Requests to /data API will be counted at 10x other requests. | | asrConcurrencyLimit | 4 | Limit on number of concurrent ASR requests. Does not apply to OFF-LINE requests. | | offlineQueueSizeLimit | 10 | Maximum number of OFF-LINE transcription jobs that may be submitted to the queue. | | offlineThroughputLimitPerHour | 4 | Maximum number of hours of audio that can be processed by OFF-LINE transcription within 1 hour. Note: For Edge deployment the limit interval is per day instead of per hour. | | offlineWorkerLimit | 2 | Maximum number of OFF-LINE transcription job workers that will be used to process the account audio. |  For API requests running longer that the rate limit window length, the request count will be applied to both the window when the request started and the window when the request finished.   Every HTTP API request will return several rate-limit related headers in its response.  The header values show the applicable limit, the remaining request count in the current window, and the number of seconds to when the limit resets. For example:  ``` RateLimit-Limit: 75, 75;window=60, 2000;window=3600 RateLimit-Remaining: 1 RateLimit-Reset: 7 ```  ## When Rate Limits are Hit  If a rate-limit is hit then [429 Too Many Requests](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429) HTTP error code will be returned. The response headers will additionally include Retry-After value, for example:  ``` RateLimit-Limit: 75, 75;window=60, 2000;window=3600 RateLimit-Remaining: 0 RateLimit-Reset: 6 Retry-After: 6 ``` If `asrConcurrencyLimit` is hit then the response headers will contain:  ``` X-ResourceLimit-Type: ASR-Concurrency X-ResourceLimit-Limit: 4 RateLimit-Limit: 0 RateLimit-Remaining: 0 RateLimit-Reset: 120 Retry-After: 120 ``` Note that we return a superset of values that are returned for a basic API request limit.  This will allow a client code that was written to handle basic rate limiting to be able to handle concurrency limiting too.  Note also that for the concurrency limit the Retry-After value is approximate and is not guaranteed - so client code may have to retry multiple times. (We will return increasing back-off Retry-After values in case of the limit being hit multiple times.)   In case of `offlineQueueSizeLimit` limit we will return, for example:  ``` X-ResourceLimit-Type: Offline-Queue-Size X-ResourceLimit-Limit: 10 RateLimit-Limit: 0 RateLimit-Remaining: 0 RateLimit-Reset: 120 Retry-After: 120 ```   # Pagination  Voicegain API supports 2 methods of pagination.  ## Sequential pagination  For methods that support sequential pagination Voicegain has standardized on using the following query parameters: + start_after={object id OR nul}  + end_before={object id OR nul}  + per_page={number items per page}  If `start_after=nul` then the first page will be retrieved.</br> If `end_before=nul` then the last page will be retrieved.  `start_after` and `end_before` should not be used together.  If neither `start_after` nor `end_before` are provided, then `start_after=nul` will be assumed.  In responses, Voicegain APIs use the [Link Header standard](https://tools.ietf.org/html/rfc5988) to provide the pagination information. The following values of the `rel` field are used: self, first, prev, next, last.  In addition to the `Link` header, the `X-Total-Count` header is used to provide the total count of items matching a query.  An example response header might look like (note: we have broken the Link header in multiple lines for readability )  ``` X-Total-Count: 255 Link: <https://api.voicegain.ai/v1/sa/call?start_after=nul&per_page=50>; rel=\"first\",       <https://api.voicegain.ai/v1/sa/call?end_before=5f7f1f7d67f67ddaa622b68e&per_page=50>; rel=\"prev\",       <https://api.voicegain.ai/v1/sa/call?start_after=5f7f1f7d67f67ddaa622b68d&per_page=50>; rel=\"self\",       <https://api.voicegain.ai/v1/sa/call?start_after=5f7f1f7d67f67ddaa622b68c4&per_page=50>; rel=\"next\",       <https://api.voicegain.ai/v1/sa/call?end_before=nul&per_page=50>; rel=\"last\" ```  ## Direct pagination  For methods that support direct pagination Voicegain has standardized on using the following query parameters: + page={page number}  + per_page={number items per page}  `page` is the page number starting from 1 (i.e. first page is 1). This is not an item offset.  This also implies that `per_page` should be kept constant for a set of related requests.  In responses, Voicegain APIs use the [Link Header standard](https://tools.ietf.org/html/rfc5988) to provide the pagination information. The following values of the `rel` field are used: self, first, prev, next, last.  In addition to the `Link` header, the `X-Total-Count` header is used to provide the total count of items matching a query.  An example response header might look like (note: we have broken the Link header in multiple lines for readability )  ``` X-Total-Count: 786 Link: <https://api.voicegain.ai/v1/sa/call?pager=1&per_page=50>; rel=\"first\",       <https://api.voicegain.ai/v1/sa/call?page=6&per_page=50>; rel=\"prev\",       <https://api.voicegain.ai/v1/sa/call?page=7&per_page=50>; rel=\"self\",       <https://api.voicegain.ai/v1/sa/call?page=8&per_page=50>; rel=\"next\",       <https://api.voicegain.ai/v1/sa/call?page=16&per_page=50>; rel=\"last\" ```   # PCI-DSS Compliance (Cloud)  The PCI-DSS compliant endpoint on the Voicegain Cloud is https://sapi.voicegain.ai/v1/ </br> Do not submit requests that may contain CHD data to the standard endpoint at https://api.voicegain.ai/v1/  Here is a list of all API Methods that are PCI-DSS compliant: + `/asr/transcribe`: [POST](#operation/asrTranscribePost) + `/asr/transcribe/async`: [POST](#operation/asrTranscribeAsyncPost) - we support OFF-LINE and REAL-TIME + `/asr/transcribe/{sessionId}`: [GET](#operation/asrTranscribeAsyncGet) [PUT](#operation/asrTranscribeAsyncPut) [DELETE](#operation/asrTranscribeAsyncDelete)  Note that the /data API is not yet PCI-DSS compliant on the Cloud. This means that the only PCI-DSS compliant ways to submit the audio are: + `fromUrl` - use `authConf` for authenticated access or use signed short-lived URLs + `inline` + `stream` - only `WSS` (old `WEBSOCKET`) and `TWIML` protocols are supported right now  https://sapi.voicegain.ai/v1/ endpoint does not support API methods that would store data, either the audio or the transcription results.   https://sapi.voicegain.ai/v1/ endpoint does support audio redaction. Redacted audio is not stored but submitted directly to the URL specified in the request `audio.callback`.   # PCI-DSS Compliance (Edge)  Because the Edge deployment happens ultimately in the customer's environment, it will the customer's responsibility to certify their Edge depoyment of the Voicegain platform as PCI-DSS compliant.  Voicegain can provide Attestation of Compliance (AoC) for the following PCI-DSS sections as far as they releate to Voicegain Software that will be deployed on Edge: + 5. Use and regularly update anti-virus software or programs + 6. Develop and maintain secure systems and applications + 11. Regularly test security systems and processes + 12. Maintain a policy that addresses information security for all personnel  For the following PCI-DSS sections we will provide detailed data regarding implementation: + 3. Protect stored cardholder data   # noqa: E501

    The version of the OpenAPI document: 1.114.0 - updated January 16, 2025
    Contact: api.support@voicegain.ai
    Generated by: https://openapi-generator.tech
"""


from __future__ import absolute_import

__version__ = "1.0.0"

# import apis into sdk package
from voicegain_speech.api.aivr_callback_api import AivrCallbackApi
from voicegain_speech.api.aivr_ws_api import AivrWsApi
from voicegain_speech.api.asr_callback_api import AsrCallbackApi
from voicegain_speech.api.audiocodes_api import AudiocodesApi
from voicegain_speech.api.data_api import DataApi
from voicegain_speech.api.greg_api import GregApi
from voicegain_speech.api.meeting_api import MeetingApi
from voicegain_speech.api.recognize_api import RecognizeApi
from voicegain_speech.api.sa_api import SaApi
from voicegain_speech.api.sa_internal_api import SaInternalApi
from voicegain_speech.api.sa_offline_api import SaOfflineApi
from voicegain_speech.api.security_api import SecurityApi
from voicegain_speech.api.text_api import TextApi
from voicegain_speech.api.training_api import TrainingApi
from voicegain_speech.api.transcribe_api import TranscribeApi
from voicegain_speech.api.websocket_api import WebsocketApi

# import ApiClient
from voicegain_speech.api_client import ApiClient
from voicegain_speech.configuration import Configuration
from voicegain_speech.exceptions import OpenApiException
from voicegain_speech.exceptions import ApiTypeError
from voicegain_speech.exceptions import ApiValueError
from voicegain_speech.exceptions import ApiKeyError
from voicegain_speech.exceptions import ApiException
# import models into sdk package
from voicegain_speech.models.ac_control_messages import ACControlMessages
from voicegain_speech.models.ac_end import ACEnd
from voicegain_speech.models.ac_error import ACError
from voicegain_speech.models.ac_hypothesis import ACHypothesis
from voicegain_speech.models.ac_hypothesis_alternatives import ACHypothesisAlternatives
from voicegain_speech.models.ac_recognition import ACRecognition
from voicegain_speech.models.ac_recognition_alternatives import ACRecognitionAlternatives
from voicegain_speech.models.ac_response_messages import ACResponseMessages
from voicegain_speech.models.ac_start import ACStart
from voicegain_speech.models.ac_start_stt_speech_contexts import ACStartSttSpeechContexts
from voicegain_speech.models.ac_started import ACStarted
from voicegain_speech.models.ac_stop import ACStop
from voicegain_speech.models.aivr import AIVR
from voicegain_speech.models.aivr_all_of import AIVRAllOf
from voicegain_speech.models.aivr_all_of_aivr_apps import AIVRAllOfAivrApps
from voicegain_speech.models.aivr_callback_core_response import AIVRCallbackCoreResponse
from voicegain_speech.models.aivr_callback_core_response_all_of import AIVRCallbackCoreResponseAllOf
from voicegain_speech.models.aivr_callback_init_response import AIVRCallbackInitResponse
from voicegain_speech.models.aivr_callback_init_response_all_of import AIVRCallbackInitResponseAllOf
from voicegain_speech.models.aivr_callback_init_response_all_of_real_time_asr_transcribe_session import AIVRCallbackInitResponseAllOfRealTimeAsrTranscribeSession
from voicegain_speech.models.aivr_callback_response import AIVRCallbackResponse
from voicegain_speech.models.aivr_callback_response_all_of import AIVRCallbackResponseAllOf
from voicegain_speech.models.aivr_callback_response_final import AIVRCallbackResponseFinal
from voicegain_speech.models.aivr_conference_transfer import AIVRConferenceTransfer
from voicegain_speech.models.aivr_disconnect import AIVRDisconnect
from voicegain_speech.models.aivr_event import AIVREvent
from voicegain_speech.models.aivr_event_type import AIVREventType
from voicegain_speech.models.aivr_existing_session import AIVRExistingSession
from voicegain_speech.models.aivr_logic_media import AIVRLogicMedia
from voicegain_speech.models.aivr_logic_transfer import AIVRLogicTransfer
from voicegain_speech.models.aivr_logic_type import AIVRLogicType
from voicegain_speech.models.aivr_new_session import AIVRNewSession
from voicegain_speech.models.aivr_new_session_ws import AIVRNewSessionWS
from voicegain_speech.models.aivr_new_session_ws_response import AIVRNewSessionWsResponse
from voicegain_speech.models.aivr_phone_transfer import AIVRPhoneTransfer
from voicegain_speech.models.aivr_phone_transfer_warm import AIVRPhoneTransferWarm
from voicegain_speech.models.aivr_prompt import AIVRPrompt
from voicegain_speech.models.aivr_prompt_properties_audio import AIVRPromptPropertiesAudio
from voicegain_speech.models.aivr_prompt_properties_html import AIVRPromptPropertiesHtml
from voicegain_speech.models.aivr_question import AIVRQuestion
from voicegain_speech.models.aivr_recognition_result import AIVRRecognitionResult
from voicegain_speech.models.aivr_response_properties_audio import AIVRResponsePropertiesAudio
from voicegain_speech.models.aivr_response_properties_html import AIVRResponsePropertiesHtml
from voicegain_speech.models.aivrs_question_specifics import AIVRSQuestionSpecifics
from voicegain_speech.models.aivr_sub_return import AIVRSubReturn
from voicegain_speech.models.aivr_trace import AIVRTrace
from voicegain_speech.models.aivr_trace_type import AIVRTraceType
from voicegain_speech.models.aivr_transfer import AIVRTransfer
from voicegain_speech.models.aivr_vars import AIVRVars
from voicegain_speech.models.aivr_vars_changed import AIVRVarsChanged
from voicegain_speech.models.aivr_warm_transfer import AIVRWarmTransfer
from voicegain_speech.models.account_and_context_id import AccountAndContextId
from voicegain_speech.models.advanced_regex import AdvancedRegex
from voicegain_speech.models.agent import Agent
from voicegain_speech.models.agent_all_of import AgentAllOf
from voicegain_speech.models.agent_base import AgentBase
from voicegain_speech.models.aircall import Aircall
from voicegain_speech.models.aircall_all_of import AircallAllOf
from voicegain_speech.models.aivr_event_with_vars import AivrEventWithVars
from voicegain_speech.models.aivr_event_with_vars_all_of import AivrEventWithVarsAllOf
from voicegain_speech.models.and_query import AndQuery
from voicegain_speech.models.and_query_all_of import AndQueryAllOf
from voicegain_speech.models.asr_processing_event import AsrProcessingEvent
from voicegain_speech.models.asr_processing_status import AsrProcessingStatus
from voicegain_speech.models.asr_processing_status_additional import AsrProcessingStatusAdditional
from voicegain_speech.models.asr_processing_status_after_input_started import AsrProcessingStatusAfterInputStarted
from voicegain_speech.models.asr_processing_status_for_callback import AsrProcessingStatusForCallback
from voicegain_speech.models.asr_recognition_result import AsrRecognitionResult
from voicegain_speech.models.asr_settings_common import AsrSettingsCommon
from voicegain_speech.models.asr_settings_common_language_detection import AsrSettingsCommonLanguageDetection
from voicegain_speech.models.asr_settings_meeting_transcription import AsrSettingsMeetingTranscription
from voicegain_speech.models.asr_settings_meeting_transcription_language_detection import AsrSettingsMeetingTranscriptionLanguageDetection
from voicegain_speech.models.asr_settings_recognition import AsrSettingsRecognition
from voicegain_speech.models.asr_settings_recognition_defaults import AsrSettingsRecognitionDefaults
from voicegain_speech.models.asr_settings_recognition_grammars_etc import AsrSettingsRecognitionGrammarsEtc
from voicegain_speech.models.asr_settings_recognition_timeouts import AsrSettingsRecognitionTimeouts
from voicegain_speech.models.asr_settings_transcription import AsrSettingsTranscription
from voicegain_speech.models.asr_settings_transcription_async import AsrSettingsTranscriptionAsync
from voicegain_speech.models.asr_settings_transcription_async_all_of import AsrSettingsTranscriptionAsyncAllOf
from voicegain_speech.models.asr_settings_transcription_common import AsrSettingsTranscriptionCommon
from voicegain_speech.models.asr_settings_transcription_common_lm import AsrSettingsTranscriptionCommonLM
from voicegain_speech.models.asr_settings_transcription_default_timeouts import AsrSettingsTranscriptionDefaultTimeouts
from voicegain_speech.models.asr_settings_transcription_defaults import AsrSettingsTranscriptionDefaults
from voicegain_speech.models.asr_settings_transcription_sa import AsrSettingsTranscriptionSA
from voicegain_speech.models.asr_settings_transcription_speakers import AsrSettingsTranscriptionSpeakers
from voicegain_speech.models.asr_settings_transcription_speakers_diarization import AsrSettingsTranscriptionSpeakersDiarization
from voicegain_speech.models.asr_transcribe_queue_status import AsrTranscribeQueueStatus
from voicegain_speech.models.async_audio_input_source import AsyncAudioInputSource
from voicegain_speech.models.async_mode import AsyncMode
from voicegain_speech.models.async_mode_recognition import AsyncModeRecognition
from voicegain_speech.models.async_mode_speech_analytics import AsyncModeSpeechAnalytics
from voicegain_speech.models.async_mode_transcription import AsyncModeTranscription
from voicegain_speech.models.async_post_response_base import AsyncPostResponseBase
from voicegain_speech.models.async_post_response_base_audio import AsyncPostResponseBaseAudio
from voicegain_speech.models.async_reco_post_response import AsyncRecoPostResponse
from voicegain_speech.models.async_reco_post_response_sessions import AsyncRecoPostResponseSessions
from voicegain_speech.models.async_recognition_callback_response import AsyncRecognitionCallbackResponse
from voicegain_speech.models.async_recognition_request import AsyncRecognitionRequest
from voicegain_speech.models.async_recognition_response import AsyncRecognitionResponse
from voicegain_speech.models.async_recognition_result import AsyncRecognitionResult
from voicegain_speech.models.async_recognition_result_all_of import AsyncRecognitionResultAllOf
from voicegain_speech.models.async_result_full import AsyncResultFull
from voicegain_speech.models.async_result_full_all_of import AsyncResultFullAllOf
from voicegain_speech.models.async_result_full_all_of_audio import AsyncResultFullAllOfAudio
from voicegain_speech.models.async_result_full_all_of_audio_callback import AsyncResultFullAllOfAudioCallback
from voicegain_speech.models.async_result_full_all_of_audio_source import AsyncResultFullAllOfAudioSource
from voicegain_speech.models.async_result_full_all_of_audio_source_data_store import AsyncResultFullAllOfAudioSourceDataStore
from voicegain_speech.models.async_result_full_all_of_result import AsyncResultFullAllOfResult
from voicegain_speech.models.async_result_incremental import AsyncResultIncremental
from voicegain_speech.models.async_result_incremental_detail import AsyncResultIncrementalDetail
from voicegain_speech.models.async_result_incremental_detail_control_status import AsyncResultIncrementalDetailControlStatus
from voicegain_speech.models.async_result_incremental_detail_result import AsyncResultIncrementalDetailResult
from voicegain_speech.models.async_result_incremental_detail_result_incremental_transcript import AsyncResultIncrementalDetailResultIncrementalTranscript
from voicegain_speech.models.async_session_established import AsyncSessionEstablished
from voicegain_speech.models.async_session_short_info import AsyncSessionShortInfo
from voicegain_speech.models.async_session_url import AsyncSessionUrl
from voicegain_speech.models.async_transc_post_response import AsyncTranscPostResponse
from voicegain_speech.models.async_transc_post_response_sessions import AsyncTranscPostResponseSessions
from voicegain_speech.models.async_transc_session_established import AsyncTranscSessionEstablished
from voicegain_speech.models.async_transcription_callback_response import AsyncTranscriptionCallbackResponse
from voicegain_speech.models.async_transcription_request import AsyncTranscriptionRequest
from voicegain_speech.models.async_transcription_response import AsyncTranscriptionResponse
from voicegain_speech.models.async_transcription_response_shared import AsyncTranscriptionResponseShared
from voicegain_speech.models.audio_channel import AudioChannel
from voicegain_speech.models.audio_channel_selector import AudioChannelSelector
from voicegain_speech.models.audio_channel_selector_offline_async import AudioChannelSelectorOfflineAsync
from voicegain_speech.models.audio_channels import AudioChannels
from voicegain_speech.models.audio_for_data_object import AudioForDataObject
from voicegain_speech.models.audio_format import AudioFormat
from voicegain_speech.models.audio_input_async import AudioInputAsync
from voicegain_speech.models.audio_input_async_source import AudioInputAsyncSource
from voicegain_speech.models.audio_input_async_with_callback import AudioInputAsyncWithCallback
from voicegain_speech.models.audio_input_base import AudioInputBase
from voicegain_speech.models.audio_input_callback import AudioInputCallback
from voicegain_speech.models.audio_input_callback_callback import AudioInputCallbackCallback
from voicegain_speech.models.audio_input_data import AudioInputData
from voicegain_speech.models.audio_input_data_all_of import AudioInputDataAllOf
from voicegain_speech.models.audio_input_data_all_of_source import AudioInputDataAllOfSource
from voicegain_speech.models.audio_input_sync import AudioInputSync
from voicegain_speech.models.audio_input_sync_source import AudioInputSyncSource
from voicegain_speech.models.audio_resource_uri import AudioResourceUri
from voicegain_speech.models.audio_time_zone import AudioTimeZone
from voicegain_speech.models.audio_zone_class import AudioZoneClass
from voicegain_speech.models.audio_zone_item import AudioZoneItem
from voicegain_speech.models.builtin import BUILTIN
from voicegain_speech.models.builtin_all_of import BUILTINAllOf
from voicegain_speech.models.base_sentence_hypothesis_or_recognition import BaseSentenceHypothesisOrRecognition
from voicegain_speech.models.base_sentence_hypothesis_or_recognition_alternatives import BaseSentenceHypothesisOrRecognitionAlternatives
from voicegain_speech.models.base_stomp_word_correction import BaseStompWordCorrection
from voicegain_speech.models.base_stomp_word_correction_without_spk import BaseStompWordCorrectionWithoutSpk
from voicegain_speech.models.base_term import BaseTerm
from voicegain_speech.models.basic_success_response import BasicSuccessResponse
from voicegain_speech.models.c_and_query import CAndQuery
from voicegain_speech.models.c_and_query_all_of import CAndQueryAllOf
from voicegain_speech.models.c_base_term import CBaseTerm
from voicegain_speech.models.c_eq_term import CEqTerm
from voicegain_speech.models.c_ge_term import CGeTerm
from voicegain_speech.models.c_gt_term import CGtTerm
from voicegain_speech.models.c_in_term import CInTerm
from voicegain_speech.models.c_le_term import CLeTerm
from voicegain_speech.models.c_lt_term import CLtTerm
from voicegain_speech.models.c_ne_term import CNeTerm
from voicegain_speech.models.c_not_query import CNotQuery
from voicegain_speech.models.c_not_query_all_of import CNotQueryAllOf
from voicegain_speech.models.c_or_query import COrQuery
from voicegain_speech.models.c_or_query_all_of import COrQueryAllOf
from voicegain_speech.models.c_query import CQuery
from voicegain_speech.models.c_rel_time_term import CRelTimeTerm
from voicegain_speech.models.c_rel_time_term_all_of import CRelTimeTermAllOf
from voicegain_speech.models.c_txt_search_term import CTxtSearchTerm
from voicegain_speech.models.c_txt_search_term_all_of import CTxtSearchTermAllOf
from voicegain_speech.models.call_attributes import CallAttributes
from voicegain_speech.models.call_field import CallField
from voicegain_speech.models.call_field_for_time import CallFieldForTime
from voicegain_speech.models.call_field_for_txt_search import CallFieldForTxtSearch
from voicegain_speech.models.call_marker import CallMarker
from voicegain_speech.models.call_search_field import CallSearchField
from voicegain_speech.models.callback_req import CallbackReq
from voicegain_speech.models.callback_req_reco import CallbackReqReco
from voicegain_speech.models.callback_resp import CallbackResp
from voicegain_speech.models.caption import Caption
from voicegain_speech.models.compliance_settings import ComplianceSettings
from voicegain_speech.models.config_value_status import ConfigValueStatus
from voicegain_speech.models.content_type import ContentType
from voicegain_speech.models.continuous_recognition import ContinuousRecognition
from voicegain_speech.models.creating_entity import CreatingEntity
from voicegain_speech.models.criterion_config import CriterionConfig
from voicegain_speech.models.criterion_satisfied import CriterionSatisfied
from voicegain_speech.models.data_obj_ref import DataObjRef
from voicegain_speech.models.data_object import DataObject
from voicegain_speech.models.data_object_all_of import DataObjectAllOf
from voicegain_speech.models.data_object_base import DataObjectBase
from voicegain_speech.models.data_object_ids import DataObjectIds
from voicegain_speech.models.data_object_no_sos_ref import DataObjectNoSosRef
from voicegain_speech.models.data_object_no_sos_ref_presigned_s3 import DataObjectNoSosRefPresignedS3
from voicegain_speech.models.data_object_no_sos_ref_presigned_s3_all_of import DataObjectNoSosRefPresignedS3AllOf
from voicegain_speech.models.data_object_with_audio import DataObjectWithAudio
from voicegain_speech.models.debug_info import DebugInfo
from voicegain_speech.models.debug_settings import DebugSettings
from voicegain_speech.models.demo import Demo
from voicegain_speech.models.demo_all_of import DemoAllOf
from voicegain_speech.models.diarization_audio_time_zone_item import DiarizationAudioTimeZoneItem
from voicegain_speech.models.diarization_data import DiarizationData
from voicegain_speech.models.diarization_zone import DiarizationZone
from voicegain_speech.models.diarization_zone_item import DiarizationZoneItem
from voicegain_speech.models.disconnect import Disconnect
from voicegain_speech.models.disconnect_all_of import DisconnectAllOf
from voicegain_speech.models.dtmf_event import DtmfEvent
from voicegain_speech.models.dtmf_event_with_channel import DtmfEventWithChannel
from voicegain_speech.models.dtmf_event_with_channel_all_of import DtmfEventWithChannelAllOf
from voicegain_speech.models.dtmf_settings import DtmfSettings
from voicegain_speech.models.eq_term import EqTerm
from voicegain_speech.models.eq_term_all_of import EqTermAllOf
from voicegain_speech.models.error import Error
from voicegain_speech.models.error_all_of import ErrorAllOf
from voicegain_speech.models.error_info import ErrorInfo
from voicegain_speech.models.estimated_queue_wait import EstimatedQueueWait
from voicegain_speech.models.experiment_platform import ExperimentPlatform
from voicegain_speech.models.extended_summary_item import ExtendedSummaryItem
from voicegain_speech.models.file_location import FileLocation
from voicegain_speech.models.formatter import Formatter
from voicegain_speech.models.greg import GREG
from voicegain_speech.models.greg_all_of import GREGAllOf
from voicegain_speech.models.grxml import GRXML
from voicegain_speech.models.grxml_all_of import GRXMLAllOf
from voicegain_speech.models.ge_term import GeTerm
from voicegain_speech.models.ge_term_all_of import GeTermAllOf
from voicegain_speech.models.generic_response import GenericResponse
from voicegain_speech.models.grammar import Grammar
from voicegain_speech.models.greg_audio import GregAudio
from voicegain_speech.models.greg_audio_all_of import GregAudioAllOf
from voicegain_speech.models.greg_audio_base import GregAudioBase
from voicegain_speech.models.greg_audio_base_with_audio import GregAudioBaseWithAudio
from voicegain_speech.models.greg_audio_id import GregAudioId
from voicegain_speech.models.greg_audio_input import GregAudioInput
from voicegain_speech.models.greg_audio_input_audio_hash import GregAudioInputAudioHash
from voicegain_speech.models.greg_audio_input_audio_id import GregAudioInputAudioId
from voicegain_speech.models.greg_audio_input_data import GregAudioInputData
from voicegain_speech.models.greg_audio_set import GregAudioSet
from voicegain_speech.models.greg_audio_set_base import GregAudioSetBase
from voicegain_speech.models.greg_audio_set_base_inclusive import GregAudioSetBaseInclusive
from voicegain_speech.models.greg_audio_set_core import GregAudioSetCore
from voicegain_speech.models.greg_audio_set_id import GregAudioSetId
from voicegain_speech.models.greg_audio_set_inclusive import GregAudioSetInclusive
from voicegain_speech.models.greg_audio_set_inclusive_core import GregAudioSetInclusiveCore
from voicegain_speech.models.greg_audio_set_inner import GregAudioSetInner
from voicegain_speech.models.greg_audio_set_response import GregAudioSetResponse
from voicegain_speech.models.greg_audio_thin import GregAudioThin
from voicegain_speech.models.greg_experiment import GregExperiment
from voicegain_speech.models.greg_experiment_base import GregExperimentBase
from voicegain_speech.models.greg_experiment_base_inclusive import GregExperimentBaseInclusive
from voicegain_speech.models.greg_experiment_base_platform_data import GregExperimentBasePlatformData
from voicegain_speech.models.greg_experiment_id import GregExperimentId
from voicegain_speech.models.greg_experiment_inclusive import GregExperimentInclusive
from voicegain_speech.models.greg_experiment_modifiable import GregExperimentModifiable
from voicegain_speech.models.greg_experiment_platform_external_asr import GregExperimentPlatformExternalASR
from voicegain_speech.models.greg_experiment_platform_upload import GregExperimentPlatformUpload
from voicegain_speech.models.greg_experiment_platform_voicegain import GregExperimentPlatformVoicegain
from voicegain_speech.models.greg_experiment_response import GregExperimentResponse
from voicegain_speech.models.greg_experiment_status import GregExperimentStatus
from voicegain_speech.models.greg_experiment_status_modifiable import GregExperimentStatusModifiable
from voicegain_speech.models.greg_grammar import GregGrammar
from voicegain_speech.models.greg_grammar_base import GregGrammarBase
from voicegain_speech.models.greg_grammar_base_light import GregGrammarBaseLight
from voicegain_speech.models.greg_grammar_id import GregGrammarId
from voicegain_speech.models.greg_grammar_inner import GregGrammarInner
from voicegain_speech.models.greg_grammar_light import GregGrammarLight
from voicegain_speech.models.greg_interpretation import GregInterpretation
from voicegain_speech.models.greg_question import GregQuestion
from voicegain_speech.models.greg_question_base import GregQuestionBase
from voicegain_speech.models.greg_question_id import GregQuestionId
from voicegain_speech.models.greg_question_inner import GregQuestionInner
from voicegain_speech.models.greg_recog_base_no_exp_nlsml_core import GregRecogBaseNoExpNlsmlCore
from voicegain_speech.models.greg_recog_base_no_exp_obj_core import GregRecogBaseNoExpObjCore
from voicegain_speech.models.greg_recog_base_no_exp_obj_or_nlsml import GregRecogBaseNoExpObjOrNlsml
from voicegain_speech.models.greg_recog_base_obj_or_nlsml import GregRecogBaseObjOrNlsml
from voicegain_speech.models.greg_recog_base_with_exp import GregRecogBaseWithExp
from voicegain_speech.models.greg_recognition import GregRecognition
from voicegain_speech.models.greg_recognition_base import GregRecognitionBase
from voicegain_speech.models.greg_recognition_id import GregRecognitionId
from voicegain_speech.models.greg_review_status import GregReviewStatus
from voicegain_speech.models.greg_source_of_truth import GregSourceOfTruth
from voicegain_speech.models.greg_truth_update import GregTruthUpdate
from voicegain_speech.models.greg_truth_updates import GregTruthUpdates
from voicegain_speech.models.gt_term import GtTerm
from voicegain_speech.models.gt_term_all_of import GtTermAllOf
from voicegain_speech.models.gui_input import GuiInput
from voicegain_speech.models.hangup import Hangup
from voicegain_speech.models.html_checkbox import HtmlCheckbox
from voicegain_speech.models.html_choice_item import HtmlChoiceItem
from voicegain_speech.models.html_radio_buttons import HtmlRadioButtons
from voicegain_speech.models.html_text_entry import HtmlTextEntry
from voicegain_speech.models.if_exists import IfExists
from voicegain_speech.models.in_term import InTerm
from voicegain_speech.models.in_term_all_of import InTermAllOf
from voicegain_speech.models.incident import Incident
from voicegain_speech.models.inline_data import InlineData
from voicegain_speech.models.inline_object import InlineObject
from voicegain_speech.models.inline_object1 import InlineObject1
from voicegain_speech.models.input import Input
from voicegain_speech.models.input_all_of import InputAllOf
from voicegain_speech.models.integration import Integration
from voicegain_speech.models.jjsgf import JJSGF
from voicegain_speech.models.jjsgf_all_of import JJSGFAllOf
from voicegain_speech.models.joined_meeting_event import JoinedMeetingEvent
from voicegain_speech.models.keyword_spot_example import KeywordSpotExample
from voicegain_speech.models.keyword_spot_group import KeywordSpotGroup
from voicegain_speech.models.keyword_spot_item import KeywordSpotItem
from voicegain_speech.models.language import Language
from voicegain_speech.models.le_term import LeTerm
from voicegain_speech.models.le_term_all_of import LeTermAllOf
from voicegain_speech.models.logic import Logic
from voicegain_speech.models.logic_all_of import LogicAllOf
from voicegain_speech.models.lt_term import LtTerm
from voicegain_speech.models.lt_term_all_of import LtTermAllOf
from voicegain_speech.models.mrcp_version import MRCPVersion
from voicegain_speech.models.mrc_pv1_asr_settings import MRCPv1AsrSettings
from voicegain_speech.models.mrc_pv2_asr_settings import MRCPv2AsrSettings
from voicegain_speech.models.meeting_add_audio_request import MeetingAddAudioRequest
from voicegain_speech.models.meeting_data import MeetingData
from voicegain_speech.models.meeting_data_audio_channels import MeetingDataAudioChannels
from voicegain_speech.models.meeting_data_chat import MeetingDataChat
from voicegain_speech.models.meeting_field import MeetingField
from voicegain_speech.models.meeting_field_for_time import MeetingFieldForTime
from voicegain_speech.models.meeting_field_for_txt_search import MeetingFieldForTxtSearch
from voicegain_speech.models.meeting_join_request import MeetingJoinRequest
from voicegain_speech.models.meeting_keyword_data import MeetingKeywordData
from voicegain_speech.models.meeting_minutes import MeetingMinutes
from voicegain_speech.models.meeting_minutes_section import MeetingMinutesSection
from voicegain_speech.models.meeting_phrase_data import MeetingPhraseData
from voicegain_speech.models.meeting_search_field import MeetingSearchField
from voicegain_speech.models.meeting_speaker_result import MeetingSpeakerResult
from voicegain_speech.models.meeting_transcribe_poll_result import MeetingTranscribePollResult
from voicegain_speech.models.meeting_transcribe_poll_result_all_of import MeetingTranscribePollResultAllOf
from voicegain_speech.models.meeting_transcribe_result_reference import MeetingTranscribeResultReference
from voicegain_speech.models.meeting_transcribe_search_result import MeetingTranscribeSearchResult
from voicegain_speech.models.meeting_transcribe_search_result_all_of import MeetingTranscribeSearchResultAllOf
from voicegain_speech.models.meeting_word_item_timed_with_annot import MeetingWordItemTimedWithAnnot
from voicegain_speech.models.meeting_words_item import MeetingWordsItem
from voicegain_speech.models.meeting_words_section_with_annot import MeetingWordsSectionWithAnnot
from voicegain_speech.models.meeting_words_section_words_with_annot import MeetingWordsSectionWordsWithAnnot
from voicegain_speech.models.min_max_speakers import MinMaxSpeakers
from voicegain_speech.models.min_max_speakers_diarization import MinMaxSpeakersDiarization
from voicegain_speech.models.modifiable_meeting_data import ModifiableMeetingData
from voicegain_speech.models.modifiable_meeting_data_response import ModifiableMeetingDataResponse
from voicegain_speech.models.modifiable_offline_sa_data import ModifiableOfflineSAData
from voicegain_speech.models.modifiable_offline_sa_data_response import ModifiableOfflineSADataResponse
from voicegain_speech.models.modifiable_transcript_data import ModifiableTranscriptData
from voicegain_speech.models.modifiable_transcript_data_response import ModifiableTranscriptDataResponse
from voicegain_speech.models.modifiable_transcript_data_response_counts import ModifiableTranscriptDataResponseCounts
from voicegain_speech.models.modifiable_transcript_data_response_rms import ModifiableTranscriptDataResponseRms
from voicegain_speech.models.modifiable_transcript_data_vad import ModifiableTranscriptDataVad
from voicegain_speech.models.mood_type import MoodType
from voicegain_speech.models.multipart_form_data_field import MultipartFormDataField
from voicegain_speech.models.name_value_pair import NameValuePair
from voicegain_speech.models.named_entity_concept import NamedEntityConcept
from voicegain_speech.models.named_entity_type import NamedEntityType
from voicegain_speech.models.named_entity_type_redactable import NamedEntityTypeRedactable
from voicegain_speech.models.named_speaker import NamedSpeaker
from voicegain_speech.models.ne_term import NeTerm
from voicegain_speech.models.ne_term_all_of import NeTermAllOf
from voicegain_speech.models.new_speech_analytics_session import NewSpeechAnalyticsSession
from voicegain_speech.models.new_speech_analytics_session_response import NewSpeechAnalyticsSessionResponse
from voicegain_speech.models.new_speech_analytics_session_response_poll import NewSpeechAnalyticsSessionResponsePoll
from voicegain_speech.models.new_speech_analytics_session_response_websocket import NewSpeechAnalyticsSessionResponseWebsocket
from voicegain_speech.models.non_session_error_response import NonSessionErrorResponse
from voicegain_speech.models.non_session_error_response400 import NonSessionErrorResponse400
from voicegain_speech.models.non_session_error_response401 import NonSessionErrorResponse401
from voicegain_speech.models.non_speaking_participant import NonSpeakingParticipant
from voicegain_speech.models.non_speaking_participant_with_optional_email import NonSpeakingParticipantWithOptionalEmail
from voicegain_speech.models.non_speaking_participant_with_optional_email_all_of import NonSpeakingParticipantWithOptionalEmailAllOf
from voicegain_speech.models.non_speaking_participant_with_optional_email_simple import NonSpeakingParticipantWithOptionalEmailSimple
from voicegain_speech.models.non_speaking_participant_with_optional_email_simple_all_of import NonSpeakingParticipantWithOptionalEmailSimpleAllOf
from voicegain_speech.models.not_query import NotQuery
from voicegain_speech.models.not_query_all_of import NotQueryAllOf
from voicegain_speech.models.offline_progress import OfflineProgress
from voicegain_speech.models.offline_sa_speaker_result import OfflineSASpeakerResult
from voicegain_speech.models.offline_sa_speaker_result_base import OfflineSASpeakerResultBase
from voicegain_speech.models.offline_sa_speaker_result_detail import OfflineSASpeakerResultDetail
from voicegain_speech.models.offline_speech_analytics_audio_request import OfflineSpeechAnalyticsAudioRequest
from voicegain_speech.models.offline_speech_analytics_audio_request_audio import OfflineSpeechAnalyticsAudioRequestAudio
from voicegain_speech.models.offline_speech_analytics_audio_request_mask import OfflineSpeechAnalyticsAudioRequestMask
from voicegain_speech.models.offline_speech_analytics_base_request import OfflineSpeechAnalyticsBaseRequest
from voicegain_speech.models.offline_speech_analytics_base_request_all_of import OfflineSpeechAnalyticsBaseRequestAllOf
from voicegain_speech.models.offline_speech_analytics_base_request_without_speakers import OfflineSpeechAnalyticsBaseRequestWithoutSpeakers
from voicegain_speech.models.offline_speech_analytics_base_result import OfflineSpeechAnalyticsBaseResult
from voicegain_speech.models.offline_speech_analytics_core_result import OfflineSpeechAnalyticsCoreResult
from voicegain_speech.models.offline_speech_analytics_core_result_all_of import OfflineSpeechAnalyticsCoreResultAllOf
from voicegain_speech.models.offline_speech_analytics_core_result_response import OfflineSpeechAnalyticsCoreResultResponse
from voicegain_speech.models.offline_speech_analytics_core_result_response_all_of import OfflineSpeechAnalyticsCoreResultResponseAllOf
from voicegain_speech.models.offline_speech_analytics_emotion_data import OfflineSpeechAnalyticsEmotionData
from voicegain_speech.models.offline_speech_analytics_emotion_item import OfflineSpeechAnalyticsEmotionItem
from voicegain_speech.models.offline_speech_analytics_request import OfflineSpeechAnalyticsRequest
from voicegain_speech.models.offline_speech_analytics_response import OfflineSpeechAnalyticsResponse
from voicegain_speech.models.offline_speech_analytics_result import OfflineSpeechAnalyticsResult
from voicegain_speech.models.offline_speech_analytics_result_detail import OfflineSpeechAnalyticsResultDetail
from voicegain_speech.models.offline_speech_analytics_result_detail_all_of import OfflineSpeechAnalyticsResultDetailAllOf
from voicegain_speech.models.offline_speech_analytics_result_detail_without_words import OfflineSpeechAnalyticsResultDetailWithoutWords
from voicegain_speech.models.or_query import OrQuery
from voicegain_speech.models.or_query_all_of import OrQueryAllOf
from voicegain_speech.models.output import Output
from voicegain_speech.models.output_all_of import OutputAllOf
from voicegain_speech.models.overtalk import Overtalk
from voicegain_speech.models.pii_redaction_conf import PIIRedactionConf
from voicegain_speech.models.pii_text_redaction_conf import PIITextRedactionConf
from voicegain_speech.models.performed_redaction import PerformedRedaction
from voicegain_speech.models.performed_redaction_all_of import PerformedRedactionAllOf
from voicegain_speech.models.performed_redaction_base import PerformedRedactionBase
from voicegain_speech.models.performed_text_redaction import PerformedTextRedaction
from voicegain_speech.models.performed_text_redaction_all_of import PerformedTextRedactionAllOf
from voicegain_speech.models.phone_audio_input import PhoneAudioInput
from voicegain_speech.models.phone_audio_input_prompt import PhoneAudioInputPrompt
from voicegain_speech.models.phrase_spot_example import PhraseSpotExample
from voicegain_speech.models.phrase_spot_group import PhraseSpotGroup
from voicegain_speech.models.phrase_spot_item import PhraseSpotItem
from voicegain_speech.models.phrase_spot_item_location import PhraseSpotItemLocation
from voicegain_speech.models.phrase_spot_item_location_dialogue import PhraseSpotItemLocationDialogue
from voicegain_speech.models.phrase_spot_item_slots import PhraseSpotItemSlots
from voicegain_speech.models.poll_req import PollReq
from voicegain_speech.models.poll_resp import PollResp
from voicegain_speech.models.portal_output_init import PortalOutputInit
from voicegain_speech.models.pre_fetch import PreFetch
from voicegain_speech.models.presigned_data_file_url_response import PresignedDataFileUrlResponse
from voicegain_speech.models.progress import Progress
from voicegain_speech.models.progress_callback import ProgressCallback
from voicegain_speech.models.progress_phase import ProgressPhase
from voicegain_speech.models.quartiles_energy import QuartilesEnergy
from voicegain_speech.models.quartiles_pitch import QuartilesPitch
from voicegain_speech.models.query import Query
from voicegain_speech.models.queue import Queue
from voicegain_speech.models.recent_period import RecentPeriod
from voicegain_speech.models.reco_alt import RecoAlt
from voicegain_speech.models.recog_nlsml import RecogNlsml
from voicegain_speech.models.recog_nlsml_no_exp import RecogNlsmlNoExp
from voicegain_speech.models.recog_obj import RecogObj
from voicegain_speech.models.recog_obj_no_exp import RecogObjNoExp
from voicegain_speech.models.recognition_result import RecognitionResult
from voicegain_speech.models.redactor import Redactor
from voicegain_speech.models.rel_time_term import RelTimeTerm
from voicegain_speech.models.rel_time_term_all_of import RelTimeTermAllOf
from voicegain_speech.models.requested_content import RequestedContent
from voicegain_speech.models.resource_uri import ResourceUri
from voicegain_speech.models.resource_uri_local_auth_conf import ResourceUriLocalAuthConf
from voicegain_speech.models.results_websocket_mode import ResultsWebsocketMode
from voicegain_speech.models.s3 import S3
from voicegain_speech.models.s3_all_of import S3AllOf
from voicegain_speech.models.s3_audio_input import S3AudioInput
from voicegain_speech.models.s3_metadata_mapping import S3MetadataMapping
from voicegain_speech.models.s3_tag_mapping import S3TagMapping
from voicegain_speech.models.sa_conf_type import SAConfType
from voicegain_speech.models.sample_rate import SampleRate
from voicegain_speech.models.sentence_hypothesis_or_recognition import SentenceHypothesisOrRecognition
from voicegain_speech.models.sentence_hypothesis_or_recognition_all_of import SentenceHypothesisOrRecognitionAllOf
from voicegain_speech.models.sentence_recognition import SentenceRecognition
from voicegain_speech.models.session_content import SessionContent
from voicegain_speech.models.session_error_response import SessionErrorResponse
from voicegain_speech.models.session_init_recognition import SessionInitRecognition
from voicegain_speech.models.session_init_transcription import SessionInitTranscription
from voicegain_speech.models.session_init_transcription_diarization import SessionInitTranscriptionDiarization
from voicegain_speech.models.session_success_response import SessionSuccessResponse
from voicegain_speech.models.settings_async_transcription import SettingsAsyncTranscription
from voicegain_speech.models.settings_recognition import SettingsRecognition
from voicegain_speech.models.settings_sync_transcription import SettingsSyncTranscription
from voicegain_speech.models.silence import Silence
from voicegain_speech.models.slot_entity import SlotEntity
from voicegain_speech.models.slot_keyword import SlotKeyword
from voicegain_speech.models.sos_ref import SosRef
from voicegain_speech.models.speaker_activity import SpeakerActivity
from voicegain_speech.models.speaker_result import SpeakerResult
from voicegain_speech.models.speaker_with_percent_spoken import SpeakerWithPercentSpoken
from voicegain_speech.models.speech_analytics_base_result import SpeechAnalyticsBaseResult
from voicegain_speech.models.speech_analytics_channel import SpeechAnalyticsChannel
from voicegain_speech.models.speech_analytics_channel_result import SpeechAnalyticsChannelResult
from voicegain_speech.models.speech_analytics_channel_with_transcribe import SpeechAnalyticsChannelWithTranscribe
from voicegain_speech.models.speech_analytics_config import SpeechAnalyticsConfig
from voicegain_speech.models.speech_analytics_config_identifying import SpeechAnalyticsConfigIdentifying
from voicegain_speech.models.speech_analytics_config_modifiable import SpeechAnalyticsConfigModifiable
from voicegain_speech.models.speech_analytics_config_modifiable_base import SpeechAnalyticsConfigModifiableBase
from voicegain_speech.models.speech_analytics_config_modifiable_base_meeting_minutes import SpeechAnalyticsConfigModifiableBaseMeetingMinutes
from voicegain_speech.models.speech_analytics_config_name_optional import SpeechAnalyticsConfigNameOptional
from voicegain_speech.models.speech_analytics_config_name_required import SpeechAnalyticsConfigNameRequired
from voicegain_speech.models.speech_analytics_config_what_defaults import SpeechAnalyticsConfigWhatDefaults
from voicegain_speech.models.speech_analytics_core_result import SpeechAnalyticsCoreResult
from voicegain_speech.models.speech_analytics_core_result_all_of import SpeechAnalyticsCoreResultAllOf
from voicegain_speech.models.speech_analytics_criteria_data import SpeechAnalyticsCriteriaData
from voicegain_speech.models.speech_analytics_emotion import SpeechAnalyticsEmotion
from voicegain_speech.models.speech_analytics_emotion_data import SpeechAnalyticsEmotionData
from voicegain_speech.models.speech_analytics_emotion_item import SpeechAnalyticsEmotionItem
from voicegain_speech.models.speech_analytics_input_speaker import SpeechAnalyticsInputSpeaker
from voicegain_speech.models.speech_analytics_keyword import SpeechAnalyticsKeyword
from voicegain_speech.models.speech_analytics_keyword_data import SpeechAnalyticsKeywordData
from voicegain_speech.models.speech_analytics_keyword_item import SpeechAnalyticsKeywordItem
from voicegain_speech.models.speech_analytics_keyword_item_for_phrase import SpeechAnalyticsKeywordItemForPhrase
from voicegain_speech.models.speech_analytics_named_entity import SpeechAnalyticsNamedEntity
from voicegain_speech.models.speech_analytics_named_entity_item import SpeechAnalyticsNamedEntityItem
from voicegain_speech.models.speech_analytics_named_entity_item_for_phrase import SpeechAnalyticsNamedEntityItemForPhrase
from voicegain_speech.models.speech_analytics_phrase import SpeechAnalyticsPhrase
from voicegain_speech.models.speech_analytics_phrase_data import SpeechAnalyticsPhraseData
from voicegain_speech.models.speech_analytics_phrase_group import SpeechAnalyticsPhraseGroup
from voicegain_speech.models.speech_analytics_phrase_group_data import SpeechAnalyticsPhraseGroupData
from voicegain_speech.models.speech_analytics_phrase_group_item import SpeechAnalyticsPhraseGroupItem
from voicegain_speech.models.speech_analytics_phrase_item import SpeechAnalyticsPhraseItem
from voicegain_speech.models.speech_analytics_phrase_itemfor_phrase import SpeechAnalyticsPhraseItemforPhrase
from voicegain_speech.models.speech_analytics_phrase_slots import SpeechAnalyticsPhraseSlots
from voicegain_speech.models.speech_analytics_result import SpeechAnalyticsResult
from voicegain_speech.models.speech_analytics_result_detail import SpeechAnalyticsResultDetail
from voicegain_speech.models.speech_analytics_session_modifiable import SpeechAnalyticsSessionModifiable
from voicegain_speech.models.speech_analytics_session_poll_response import SpeechAnalyticsSessionPollResponse
from voicegain_speech.models.start_end_time_for_sub_criterion import StartEndTimeForSubCriterion
from voicegain_speech.models.status_message_response import StatusMessageResponse
from voicegain_speech.models.stomp_metadata import StompMetadata
from voicegain_speech.models.stomp_ping import StompPing
from voicegain_speech.models.stomp_word_correction import StompWordCorrection
from voicegain_speech.models.stomp_word_correction_all_of import StompWordCorrectionAllOf
from voicegain_speech.models.stomp_ws_word import StompWsWord
from voicegain_speech.models.stomp_ws_word_all_of import StompWsWordAllOf
from voicegain_speech.models.stomp_ws_word_base import StompWsWordBase
from voicegain_speech.models.stomp_ws_word_base_all_of import StompWsWordBaseAllOf
from voicegain_speech.models.stomp_ws_word_base_without_spk import StompWsWordBaseWithoutSpk
from voicegain_speech.models.stream_resp import StreamResp
from voicegain_speech.models.stream_setup import StreamSetup
from voicegain_speech.models.streaming_protocol import StreamingProtocol
from voicegain_speech.models.stt_generic_data import SttGenericData
from voicegain_speech.models.stt_generic_data_settings import SttGenericDataSettings
from voicegain_speech.models.stt_generic_data_settings_asr import SttGenericDataSettingsAsr
from voicegain_speech.models.stt_generic_data_settings_asr_interpretation import SttGenericDataSettingsAsrInterpretation
from voicegain_speech.models.sub_criterion_config import SubCriterionConfig
from voicegain_speech.models.sub_criterion_satisfied import SubCriterionSatisfied
from voicegain_speech.models.sub_return import SubReturn
from voicegain_speech.models.sub_return_all_of import SubReturnAllOf
from voicegain_speech.models.sync_audio_input_source import SyncAudioInputSource
from voicegain_speech.models.sync_recognition_request import SyncRecognitionRequest
from voicegain_speech.models.sync_recognition_response import SyncRecognitionResponse
from voicegain_speech.models.sync_session_established import SyncSessionEstablished
from voicegain_speech.models.sync_transcription_request import SyncTranscriptionRequest
from voicegain_speech.models.sync_transcription_response import SyncTranscriptionResponse
from voicegain_speech.models.sync_transcription_result import SyncTranscriptionResult
from voicegain_speech.models.talk_time import TalkTime
from voicegain_speech.models.text_redaction_request import TextRedactionRequest
from voicegain_speech.models.text_redaction_response import TextRedactionResponse
from voicegain_speech.models.timed_sentence import TimedSentence
from voicegain_speech.models.timeline_speaker import TimelineSpeaker
from voicegain_speech.models.timeline_speaker_with_email import TimelineSpeakerWithEmail
from voicegain_speech.models.timeline_speaker_with_email_all_of import TimelineSpeakerWithEmailAllOf
from voicegain_speech.models.topic_score import TopicScore
from voicegain_speech.models.trace_copilot_in import TraceCopilotIn
from voicegain_speech.models.trace_copilot_in_all_of import TraceCopilotInAllOf
from voicegain_speech.models.trace_copilot_out import TraceCopilotOut
from voicegain_speech.models.trace_copilot_out_all_of import TraceCopilotOutAllOf
from voicegain_speech.models.trace_transfer import TraceTransfer
from voicegain_speech.models.trace_transfer_all_of import TraceTransferAllOf
from voicegain_speech.models.training_set_bucket_type import TrainingSetBucketType
from voicegain_speech.models.training_set_doc import TrainingSetDoc
from voicegain_speech.models.training_set_doc_defaults import TrainingSetDocDefaults
from voicegain_speech.models.training_set_doc_statistics import TrainingSetDocStatistics
from voicegain_speech.models.training_set_modifiable import TrainingSetModifiable
from voicegain_speech.models.training_set_status import TrainingSetStatus
from voicegain_speech.models.training_set_status_modifiable import TrainingSetStatusModifiable
from voicegain_speech.models.training_set_store_type import TrainingSetStoreType
from voicegain_speech.models.transcribe_alt import TranscribeAlt
from voicegain_speech.models.transcribe_meeting_request import TranscribeMeetingRequest
from voicegain_speech.models.transcribe_meeting_request_audio import TranscribeMeetingRequestAudio
from voicegain_speech.models.transcribe_meeting_request_diarization import TranscribeMeetingRequestDiarization
from voicegain_speech.models.transcribe_meeting_request_settings import TranscribeMeetingRequestSettings
from voicegain_speech.models.transcribe_meeting_request_source import TranscribeMeetingRequestSource
from voicegain_speech.models.transcribe_meeting_response import TranscribeMeetingResponse
from voicegain_speech.models.transcribe_session_id import TranscribeSessionId
from voicegain_speech.models.transcribe_session_modify_request import TranscribeSessionModifyRequest
from voicegain_speech.models.transcribe_session_modify_request_mute import TranscribeSessionModifyRequestMute
from voicegain_speech.models.transcribe_session_modify_request_pause import TranscribeSessionModifyRequestPause
from voicegain_speech.models.transcript_data import TranscriptData
from voicegain_speech.models.transcript_position_range import TranscriptPositionRange
from voicegain_speech.models.transcript_position_range_for_phrase import TranscriptPositionRangeForPhrase
from voicegain_speech.models.transcript_position_range_for_phrase_with_chn import TranscriptPositionRangeForPhraseWithChn
from voicegain_speech.models.transcript_position_range_simple import TranscriptPositionRangeSimple
from voicegain_speech.models.transfer import Transfer
from voicegain_speech.models.transfer_all_of import TransferAllOf
from voicegain_speech.models.txt_search_term import TxtSearchTerm
from voicegain_speech.models.txt_search_term_all_of import TxtSearchTermAllOf
from voicegain_speech.models.typed_sentence import TypedSentence
from voicegain_speech.models.typed_sentence_justification import TypedSentenceJustification
from voicegain_speech.models.typed_sentence_with_rejection import TypedSentenceWithRejection
from voicegain_speech.models.typed_sentence_with_rejection_all_of import TypedSentenceWithRejectionAllOf
from voicegain_speech.models.typed_sentence_with_rejection_all_of_rejection import TypedSentenceWithRejectionAllOfRejection
from voicegain_speech.models.vad_mode import VadMode
from voicegain_speech.models.version import Version
from voicegain_speech.models.voice_call import VoiceCall
from voicegain_speech.models.voice_call_all_of import VoiceCallAllOf
from voicegain_speech.models.voice_call_modifiable_base import VoiceCallModifiableBase
from voicegain_speech.models.voice_call_search_result import VoiceCallSearchResult
from voicegain_speech.models.voice_call_search_result_all_of import VoiceCallSearchResultAllOf
from voicegain_speech.models.ws_sentence_hypothesis_or_recognition import WSSentenceHypothesisOrRecognition
from voicegain_speech.models.ws_sentence_hypothesis_or_recognition_all_of import WSSentenceHypothesisOrRecognitionAllOf
from voicegain_speech.models.warm_transfer import WarmTransfer
from voicegain_speech.models.warm_transfer_all_of import WarmTransferAllOf
from voicegain_speech.models.websocket import Websocket
from voicegain_speech.models.websocket_init import WebsocketInit
from voicegain_speech.models.websocket_init_reco import WebsocketInitReco
from voicegain_speech.models.websocket_modifiable import WebsocketModifiable
from voicegain_speech.models.websocket_msg import WebsocketMsg
from voicegain_speech.models.websocket_protocol import WebsocketProtocol
from voicegain_speech.models.websocket_resp import WebsocketResp
from voicegain_speech.models.websocket_status_item import WebsocketStatusItem
from voicegain_speech.models.websocket_status_section import WebsocketStatusSection
from voicegain_speech.models.word_alternatives import WordAlternatives
from voicegain_speech.models.word_cloud_item import WordCloudItem
from voicegain_speech.models.word_correction import WordCorrection
from voicegain_speech.models.word_item_annotation import WordItemAnnotation
from voicegain_speech.models.word_item_annotations import WordItemAnnotations
from voicegain_speech.models.word_item_timed import WordItemTimed
from voicegain_speech.models.word_item_timing import WordItemTiming
from voicegain_speech.models.word_tree_ids import WordTreeIds
from voicegain_speech.models.word_tree_item import WordTreeItem
from voicegain_speech.models.words_item import WordsItem
from voicegain_speech.models.words_section import WordsSection
from voicegain_speech.models.words_section_meta import WordsSectionMeta
from voicegain_speech.models.words_section_meta_mc import WordsSectionMetaMC
from voicegain_speech.models.words_section_meta_meeting import WordsSectionMetaMeeting
from voicegain_speech.models.words_section_single_column import WordsSectionSingleColumn
from voicegain_speech.models.words_section_words import WordsSectionWords
from voicegain_speech.models.words_websocket_item import WordsWebsocketItem
from voicegain_speech.models.ws_aivr_control_messages import WsAivrControlMessages
from voicegain_speech.models.ws_aivr_control_messages_all_of import WsAivrControlMessagesAllOf
from voicegain_speech.models.ws_aivr_server_messages import WsAivrServerMessages

